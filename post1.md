## Gen AI - LLM, RAG & Fine-tuning â€“ Hiá»ƒu vÃ  á»¨ng Dá»¥ng
ChÃ o má»i ngÆ°á»i,

AI táº¡o sinh (Gen AI) Ä‘ang ngÃ y cÃ ng Ä‘Æ°á»£c á»©ng dá»¥ng rá»™ng rÃ£i, xu hÆ°á»›ng phÃ¡t triá»ƒn cá»§a AI lÃ  khÃ´ng thá»ƒ Ä‘áº£o ngÆ°á»£c, vÃ¬ tháº¿ chÃºng ta cáº§n tÃ¬m cÃ¡ch sá»­ dá»¥ng nÃ³ má»™t cÃ¡ch há»£p lÃ½ vÃ  hiá»‡u quáº£ hÆ¡n.

Vá»›i anh chá»‹ em DE chÃºng ta khÃ´ng nhá»¯ng cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ AI cÃ³ sáºµn (chatgpt, gemini, awsQ, midjourney, kling ai...) mÃ  cÃ²n cÃ³ kháº£ nÄƒng tá»± viáº¿t cÃ¡c pháº§n má»m, á»©ng dá»¥ng cá»§a riÃªng mÃ¬nh sá»­ dá»¥ng cÃ¡c model AI miá»…n phÃ­ (vÃ­ dá»¥ deepseek, llama3, qwen ...) hoáº·c tráº£ phÃ­ (gpt, claude...) vÃ  kiá»ƒm soÃ¡t Ä‘Æ°á»£c cÃ¡c model Ä‘Ã³ vá»›i dá»¯ liá»‡u cÃ¡ nhÃ¢n cá»§a mÃ¬nh, pháº£n há»“i theo ngá»¯ cáº£nh mÃ  mÃ¬nh muá»‘n.
Tháº¿ nÃªn hÃ´m nay em/mÃ¬nh muá»‘n chia sáº» má»™t chÃºt vá» chá»§ Ä‘á» Gen AI, cá»¥ thá»ƒ lÃ  LLM, RAG & Fine-tuning Ä‘á»ƒ má»i ngÆ°á»i cÃ¹ng tháº£o luáº­n, trao Ä‘á»•i kiáº¿n thá»©c, cÅ©ng lÃ  cÃ¡ch em/mÃ¬nh verify láº¡i nhá»¯ng hiá»ƒu biáº¿t cá»§a mÃ¬nh vá» lÄ©nh vá»±c nÃ y.

Hy vá»ng vá»›i nhá»¯ng chia sáº» nÃ y cÃ³ thá»ƒ giÃºp anh chá»‹ em DE tiáº¿t kiá»‡m thá»i gian, cÃ´ng sá»©c tÃ¬m hiá»ƒu vÃ  tiáº¿p cáº­n lÄ©nh vá»±c nÃ y má»™t cÃ¡ch nhanh chÃ³ng.

Náº¿u cÃ³ chá»— nÃ o cÃ²n chÆ°a Ä‘Ãºng xin má»i ngÆ°á»i Ä‘Ã³ng gÃ³p Ã½ kiáº¿n Ä‘á»ƒ em/mÃ¬nh sá»­a láº¡i nhÃ©.

## CÃ³ nhá»¯ng giáº£i phÃ¡p nÃ o Ä‘á»ƒ cÃ³ thá»ƒ táº¡o pháº§n má»m/ á»©ng dá»¥ng AI cá»§a riÃªng mÃ¬nh ?
1. Build model tá»« Ä‘áº§u (cáº§n kiáº¿n thá»©c chuyÃªn mÃ´n vÃ  lÆ°á»£ng tÃ i nguyÃªn cá»±c lá»›n) => khÃ´ng pháº£i má»¥c tiÃªu cá»§a chÃºng ta
2. Finetuning model opensource cÃ³ sáºµn => KhÃ´ng cáº§n quÃ¡ nhiá»u tÃ i nguyÃªn, máº¥t Ã­t thá»i gian hÆ¡n, cháº¡y trÃªn gpu.
3. Ãp dá»¥ng RAG(Retrieval-Augmented Generation) vÃ  tÃ¬m kiáº¿m thÃ´ng tin (retrieval) => Cáº§n ráº¥t Ã­t tÃ i nguyÃªn, cÃ³ thá»ƒ cháº¡y báº±ng cpu, cÃ³ thá»ƒ cáº­p nháº­t kiáº¿n thá»©c má»›i liÃªn tá»¥c.

Má»—i giáº£i phÃ¡p trÃªn Ä‘á»u cÃ³ Æ°u vÃ  nhÆ°á»£c Ä‘iá»ƒm riÃªng, chÃºng ta sáº½ so sÃ¡nh sau khi Ä‘Ã£ hiá»ƒu vá» cÃ¡ch triá»ƒn khai cá»§a tá»«ng giáº£i phÃ¡p.
Äá»ƒ cÃ³ thá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c cÃ¡c giáº£i phÃ¡p trÃªn, trÆ°á»›c háº¿t chÃºng ta cáº§n hiá»ƒu thÃªm vá» LLM.

## SÆ¡ lÆ°á»£c quy trÃ¬nh suy luáº­n (inference) cá»§a LLM
Háº§u háº¿t cÃ¡c model LLM hiá»‡n nay Ä‘á»u sá»­ dá»¥ng kiáº¿n trÃºc transformer nÃªn cÃ³ dÃ¹ng quy trÃ¬nh xá»­ lÃ½ nhÆ° sau:
#### Text/Prompt => Tokenize => Embedding => Attention & Transformer Layers => Decoding & Sampling => Output
Pháº§n nÃ y hÆ¡i lÃ½ thuyáº¿t nhÆ°ng em/mÃ¬nh sáº½ cá»‘ gáº¯ng diá»…n giáº£i ngáº¯n gá»n Ä‘á»ƒ má»i ngÆ°á»i hiá»ƒu nhá»¯ng bÆ°á»›c quan trá»ng Ä‘á»ƒ khi finetuning hoáº·c Ã¡p dá»¥ng RAG vÃ o model LLM sáº½ cáº§n Ä‘áº¿n.

## Giáº£i thÃ­ch cÃ¡c khÃ¡i niá»‡m á»Ÿ trÃªn vÃ  nhá»¯ng gÃ¬ xáº£y ra á»Ÿ bÆ°á»›c Ä‘Ã³
- LLM viáº¿t táº¯t cá»§a Large Language Model lÃ  cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (hÃ ng trÄƒm triá»‡u Ä‘áº¿n hÃ ng tá»‰ tham sá»‘) Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u ráº¥t lá»›n (hÃ ng nghÃ¬n tá»‰ token) trong nhiá»u giá» báº±ng ráº¥t nhiá»u gpu. Nháº­n Ä‘áº§u vÃ o lÃ  má»™t chuá»—i text vÃ  tráº£ ra káº¿t quáº£ cÃ³ thá»ƒ lÃ  chuá»—i text cÃ³ liÃªn quan Ä‘áº¿n input (lÃ m thÆ¡, tráº£ lá»i yÃªu cáº§u ...) hoáº·c xÃ¡c suáº¥t (Ä‘á»‘i vá»›i bÃ i toÃ¡n phÃ¢n lá»›p).

- Text/Prompt: lÃ  chuá»—i cÃ¡c yÃªu cáº§u ngÆ°á»i dÃ¹ng vÃ­ dá»¥: "HÃ´m nay lÃ  thá»© máº¥y?"

- Tokenize: LÃ  quÃ¡ trÃ¬nh tÃ¡ch tá»« vÃ  mÃ£ hoÃ¡ tá»« thÃ nh cÃ¡c con sá»‘ (gá»i lÃ  cÃ¡c token), vÃ­ dá»¥ "HÃ´m nay lÃ  thá»© máº¥y?" sáº½ Ä‘Æ°á»£c tÃ¡ch thÃ nh ["HÃ´m", "nay", "lÃ ", "thá»©", "máº¥y", "?"] sau Ä‘Ã³ sá»‘ hoÃ¡ thÃ nh [22,42,11,23,65,73]. Viá»‡c tokenize sáº½ Ä‘áº£m báº£o vá»›i má»—i con sá»‘ tÆ°Æ¡ng á»©ng vá»›i 1 tá»« duy nháº¥t. Tá»•ng táº¥t cáº£ bá»™ tokens cá»§a model gá»i lÃ  Vocabulary(Vocab).

- Embedding lÃ  quÃ¡ trÃ¬nh Ã¡nh xáº¡ cÃ¡c token Ä‘Ã£ mÃ£ hoÃ¡ vÃ o khÃ´ng gian vector N chiá»u. Sau khi embedding cÃ¡c tá»« cÃ³ tÃ­nh cháº¥t giá»‘ng nhau sáº½ Ä‘Æ°á»£c á»Ÿ gáº§n nhau trong khÃ´ng gian vector Ä‘Ã³ ("Ä‘áº¹p" sáº½ á»Ÿ gáº§n "xinh" vÃ  cÃ¡ch xa "xáº¥u"), vÃ­ dá»¥ Vector cá»§a token "AI" cÃ³ thá»ƒ lÃ  [0.23, -0.67, 0.89, ..., 0.12]  (768 chiá»u vá»›i GPT-3.5). VÃ¬ tÃ­nh cháº¥t nÃ y nÃªn embedding sáº½ Ä‘Æ°á»£c dÃ¹ng Ä‘á»… vector hoÃ¡ thÃ´ng tin document trong RAG sau Ä‘Ã³ sáº½ tÃ¬m kiáº¿m ngá»¯ cáº£nh dá»±a vÃ o Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a 2 vector query vÃ  document (similarity search).

- Attention & Transformer Layers: Pháº§n nÃ y khÃ¡ dÃ i, phá»©c táº¡p vÃ  khÃ´ng quÃ¡ quan trá»ng Ä‘á»ƒ cÃ³ thá»ƒ Ã¡p dá»¥ng RAG hay finetuning nÃªn má»i ngÆ°á»i cÃ³ thá»ƒ tÃ¬m hiá»ƒu thÃªm táº¡i https://www.youtube.com/watch?v=_Zt23FA31co&t=125s

- Decoding & Sampling:  á»Ÿ bÆ°á»›c nÃ y sáº½ cáº¥u hÃ¬nh Ä‘á»ƒ LLM sÃ¡ng táº¡o hÆ¡n (temperature), Giá»›i háº¡n sá»‘ lÆ°á»£ng tá»« cÃ³ xÃ¡c suáº¥t cao nháº¥t Ä‘á»ƒ lá»±a chá»n(Top-k Sampling) ... pháº§n nÃ y chá»§ yáº¿u Ä‘á»ƒ cáº¥u hÃ¬nh model khi sá»­ dá»¥ng.

- Output: CÃ¡c token sinh ra láº§n lÆ°á»£t dÆ°á»›i dáº¡ng sá»‘ Ä‘Æ°á»£c giáº£i mÃ£ (decode) vá» text vÃ  ghÃ©p láº¡i vá»›i nhau thÃ nh cÃ¢u tráº£ lá»i. Viá»‡c sinh tá»« xáº£y ra tuáº§n tá»± nhÆ° sau:

=> NgÆ°á»i dÃ¹ng input vÃ o Ä‘oáº¡n text "HÃ´m nay lÃ  thá»© máº¥y?"

=> model sinh ra token 31 tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i tá»« "Thá»©"

=> model ghÃ©p tá»« má»›i sinh vá»›i input trÆ°á»›c Ä‘Ã³ thÃ nh "HÃ´m nay lÃ  thá»© máº¥y? Thá»©" Ä‘á»ƒ lÃ m input suy luáº­n tá»« tiáº¿p theo.

=> model sinh ra token 29 tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i tá»« "ba".

=> láº·p láº¡i cho Ä‘áº¿n khi model dá»± Ä‘oÃ¡n token tiáº¿p theo lÃ  token <end> (hoáº·c má»™t token nÃ o Ä‘Ã³ khÃ¡c cÃ³ Ã½ nghÄ©a tÆ°Æ¡ng tá»± tuá»³ vocab cá»§a model) thÃ¬ model sáº½ dá»«ng láº¡i. Náº¿u input Ä‘áº¡t Ä‘áº¿n ngÆ°á»¡ng tá»‘i Ä‘a vá» token size (sá»‘ lÆ°á»£ng token input cÃ¹ng lÃºc vÃ o cho model) thÃ¬ model cÅ©ng sáº½ dá»«ng generate token má»›i.

Sau Ä‘Ã³ ghÃ©p chuá»—i cÃ³ Ä‘Æ°á»£c láº¡i vÃ  cáº¯t Ä‘i pháº§n input ban Ä‘áº§u ta Ä‘Æ°á»£c cÃ¢u tráº£ lá»i "Thá»© ba".

### Note: Tá»« viá»‡c hiá»ƒu Ä‘Æ°á»£c luá»“ng suy luáº­n cá»§a LLM chÃºng ta cÃ³ thá»ƒ rÃºt ra má»™t sá»‘ lÆ°u Ã½ nhÆ° sau:
- Model sáº½ sinh thÃªm tá»«ng tá»« má»™t chá»© khÃ´ng thá»ƒ sinh táº¥t cáº£ cÃ¹ng lÃºc.
- Text/Prompt input vÃ o cho LLM model cÃ³ giá»›i háº¡n vá» length, náº¿u chÃºng ta input Ä‘oáº¡n text quÃ¡ dÃ i cÃ³ thá»ƒ vÆ°á»£t quÃ¡ giá»›i háº¡n cá»§a model gÃ¢y lá»—i hoáº·c bá»‹ cáº¯t bá»›t ná»™i dung cá»§a Text/Prompt.
- Viá»‡c input dÃ i cho má»™t cÃ¢u há»i sáº½ lÃ m giá»›i háº¡n vá» token size Ä‘áº¡t Ä‘áº¿n sá»›m, vÃ¬ tháº¿ hÃ£y cá»‘ gáº¯ng input ngáº¯n gá»n vÃ  Ä‘á»§ Ã½, loáº¡i bá» cÃ¡c kÃ½ tá»± thá»«a trong input trÆ°á»›c khi Ä‘Æ°a vÃ o LLM suy luáº­n.
- TÆ°Æ¡ng tá»± thÃ¬ Embedding cÅ©ng cÃ³ giá»›i háº¡n vá» token size. Cáº§n chÃº Ã½ khi sá»­ dá»¥ng RAG

# Fine-tuning LLM
- LLM lÃ  neural network ráº¥t lá»›n vá»›i ráº¥t nhiá»u layer vÃ  sá»‘ lÆ°á»£ng parameters lÃªn Ä‘áº¿n hÃ ng tá»‰ vÃ¬ tháº¿ viá»‡c training vá»›i táº¥t cáº£ lÆ°á»£ng parameters Ä‘Ã³ lÃ  ráº¥t tá»‘n kÃ©m. NgÆ°á»i ta Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c ráº±ng, cÃ¡c lá»›p cÃ ng gáº§n input thÃ¬ cÃ ng mang tÃ­nh tá»•ng quÃ¡t, Ä‘áº·c trÆ°ng chung, cÃ¡c lá»›p cÃ ng gáº§n output thÃ¬ cÃ ng mang tÃ­nh riÃªng biá»‡t cho cÃ¡c bÃ i toÃ¡n cá»¥ thá»ƒ. Váº­y nÃªn kÄ© thuáº­t finetuning ra Ä‘á»i, báº±ng cÃ¡ch loáº¡i bá» Ä‘i nhá»¯ng layer gáº§n output cá»§a model, thay chÃºng báº±ng cÃ¡c layer má»›i vÃ  chá»‰ cáº­p nháº­t láº¡i cÃ¡c parameters cá»§a cÃ¡c layer má»›i Ä‘Ã³ giÃºp tiáº¿t kiá»‡m chi phÃ­ tÃ­nh toÃ¡n ráº¥t nhiá»u.

=> Tiáº¿t kiá»‡m Ä‘Ã¡ng ká»ƒ chi phÃ­ tÃ­nh toÃ¡n vÃ¬ chá»‰ cáº§n cáº­p nháº­t má»™t pháº§n nhá» cá»§a mÃ´ hÃ¬nh.

=> Giá»¯ láº¡i kiáº¿n thá»©c chung cá»§a mÃ´ hÃ¬nh gá»‘c, nhÆ°ng tinh chá»‰nh Ä‘á»ƒ phÃ¹ há»£p vá»›i tÃ¡c vá»¥ cá»¥ thá»ƒ.

=> Huáº¥n luyá»‡n nhanh hÆ¡n mÃ  váº«n duy trÃ¬ hiá»‡u suáº¥t tá»‘t trÃªn dá»¯ liá»‡u má»›i.

Note: Finetuning lÃ  ká»¹ thuáº­t cÃ³ thá»ƒ Ã¡p dá»¥ng vá»›i háº§u háº¿t neural network models khÃ´ng chá»‰ má»—i LLM

- Vá»›i LLM ngÆ°á»i ta thÆ°á»ng sá»­ dá»¥ng LORA(Low-Rank Adaptation) Ä‘á»ƒ finetuning model. Báº±ng cÃ¡ch bá»• sung thÃªm ma tráº­n cÃ³ háº¡ng tháº¥p (low rank) vÃ o pháº§n output cá»§a model sau Ä‘Ã³ tinh chá»‰nh tham sá»‘ cá»§a ma tráº­n Ä‘Ã³ vÃ  giá»¯ nguyÃªn model gá»‘c (khÃ´ng cáº§n bá» Ä‘i nhá»¯ng layer á»Ÿ gáº§n cuá»‘i) giÃºp giáº£m chi phÃ­ tÃ­nh toÃ¡n, tiáº¿t kiá»‡m bá»™ nhá»›, tÄƒng tá»‘c Ä‘á»™ huáº¥n luyá»‡n model Ä‘á»“ng thá»i cÃ³ thá»ƒ lÆ°u ma tráº­n Ä‘Ã³ láº¡i vÃ  chia sáº» vá»›i nhau.

# RAG
CÃ¡c mÃ´ hÃ¬nh LLM nhÆ° GPT-4, LLaMA, hay Deepseek-R1 cÃ³ má»™t háº¡n cháº¿ lá»›n:
- KhÃ´ng thá»ƒ cáº­p nháº­t kiáº¿n thá»©c sau khi huáº¥n luyá»‡n.

- Bá»‹ giá»›i háº¡n trong pháº¡m vi dá»¯ liá»‡u mÃ  chÃºng Ä‘Ã£ há»c.

- Tá»‘n kÃ©m náº¿u muá»‘n fine-tune láº¡i vá»›i dá»¯ liá»‡u má»›i.

Giáº£i phÃ¡p cho cÃ¡c háº¡n cháº¿ Ä‘Ã³ lÃ  sá»­ dá»¥ng RAG (Retrieval-Augmented Generation):
- Thay vÃ¬ lÆ°u trá»¯ toÃ n bá»™ kiáº¿n thá»©c trong bá»™ nhá»› cá»§a LLM, ta dÃ¹ng má»™t kho dá»¯ liá»‡u ngoÃ i (vector database) Ä‘á»ƒ lÆ°u trá»¯ tÃ i liá»‡u.

- Khi nháº­n Ä‘Æ°á»£c cÃ¢u há»i, há»‡ thá»‘ng sáº½ tÃ¬m kiáº¿m thÃ´ng tin tá»« kho dá»¯ liá»‡u, sau Ä‘Ã³ káº¿t há»£p vá»›i LLM Ä‘á»ƒ táº¡o ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c hÆ¡n.

Quy trÃ¬nh hoáº¡t Ä‘á»™ng cá»§a RAG
1. Nháº­n input tá»« ngÆ°á»i dÃ¹ng (Prompt)

VÃ­ dá»¥: "TÃ³m táº¯t bÃ i bÃ¡o khoa há»c vá» LLM RAG?"

2. Tokenize & Embed prompt

CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng Ä‘Æ°á»£c chuyá»ƒn thÃ nh vector embedding.

3. Truy váº¥n kho dá»¯ liá»‡u (Vector Store Search)

Há»‡ thá»‘ng tÃ¬m kiáº¿m tÃ i liá»‡u liÃªn quan trong kho dá»¯ liá»‡u vector (ChromaDB, Pinecone, FAISSâ€¦).

VÃ­ dá»¥: Tráº£ vá» má»™t Ä‘oáº¡n tÃ i liá»‡u cÃ³ ná»™i dung liÃªn quan Ä‘áº¿n "LLM RAG".

4. Káº¿t há»£p dá»¯ liá»‡u & LLM (Augment & Generate)

LLM sáº½ sá»­ dá»¥ng tÃ i liá»‡u tÃ¬m Ä‘Æ°á»£c Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i chÃ­nh xÃ¡c hÆ¡n.

VÃ­ dá»¥: "Dá»±a trÃªn bÃ i bÃ¡o khoa há»c, LLM RAG lÃ  má»™t mÃ´ hÃ¬nh káº¿t há»£p giá»¯aâ€¦".

5. Tráº£ káº¿t quáº£ cho ngÆ°á»i dÃ¹ng

NgÆ°á»i dÃ¹ng nháº­n Ä‘Æ°á»£c cÃ¢u tráº£ lá»i Ä‘Ãºng, cáº­p nháº­t vÃ  chi tiáº¿t hÆ¡n so vá»›i mÃ´ hÃ¬nh LLM thÃ´ng thÆ°á»ng.

# So sÃ¡nh giá»¯a Fine-tuning vÃ  RAG

| **TiÃªu chÃ­**         | **Fine-tuning** | **RAG** |
|----------------------|----------------|---------|
| **Má»¥c Ä‘Ã­ch**        | Äiá»u chá»‰nh mÃ´ hÃ¬nh Ä‘á»ƒ há»c tá»« dá»¯ liá»‡u má»›i | Káº¿t há»£p tÃ¬m kiáº¿m thÃ´ng tin vá»›i LLM Ä‘á»ƒ tráº£ lá»i chÃ­nh xÃ¡c hÆ¡n |
| **CÃ¡ch hoáº¡t Ä‘á»™ng**  | Huáº¥n luyá»‡n láº¡i mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u cá»¥ thá»ƒ | TÃ¬m kiáº¿m thÃ´ng tin trong cÆ¡ sá»Ÿ dá»¯ liá»‡u vector vÃ  káº¿t há»£p vá»›i LLM |
| **Cáº­p nháº­t dá»¯ liá»‡u má»›i** | âŒ Cáº§n huáº¥n luyá»‡n láº¡i | âœ… Chá»‰ cáº§n cáº­p nháº­t dá»¯ liá»‡u trong vector store |
| **Chi phÃ­ tÃ i nguyÃªn** | ğŸ”´ Cao (cáº§n GPU máº¡nh) | ğŸŸ¢ Tháº¥p (chá»‰ cáº§n vector database) |
| **Tá»‘c Ä‘á»™ triá»ƒn khai** | ğŸ”´ Cháº­m (vÃ i giá» Ä‘áº¿n vÃ i ngÃ y) | ğŸŸ¢ Nhanh (chá»‰ vÃ i giÃ¢y Ä‘á»ƒ thÃªm tÃ i liá»‡u má»›i) |
| **TÃ­nh linh hoáº¡t** | ğŸŸ¡ Chá»‰ giá»i trong pháº¡m vi dá»¯ liá»‡u Ä‘Ã£ fine-tune | ğŸŸ¢ CÃ³ thá»ƒ cáº­p nháº­t thÃ´ng tin má»›i liÃªn tá»¥c |
| **á»¨ng dá»¥ng phÃ¹ há»£p** | Khi cáº§n Ä‘iá»u chá»‰nh phong cÃ¡ch hoáº·c chuyÃªn sÃ¢u vá» má»™t lÄ©nh vá»±c | Khi cáº§n cáº­p nháº­t dá»¯ liá»‡u má»›i vÃ  tÃ¬m kiáº¿m thÃ´ng tin theo ngá»¯ cáº£nh |
| **VÃ­ dá»¥ sá»­ dá»¥ng** | Dáº¡y GPT cÃ¡ch viáº¿t theo phong cÃ¡ch cá»§a cÃ´ng ty | Há»i Ä‘Ã¡p dá»±a trÃªn tÃ i liá»‡u ná»™i bá»™ |

# Code example
Sau má»™t chuá»—i lÃ½ thuyáº¿t Ä‘au Ä‘áº§u thÃ¬ cÅ©ng Ä‘áº¿n lÃºc chÃºng ta tá»± táº¡o Gen AI cá»§a riÃªng mÃ¬nh :D

DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» viá»‡c Finetuning LLM vÃ  káº¿t há»£p RAG vá»›i LLM sá»­ dá»¥ng ngÃ´n ngá»¯ python.

### Táº¡i sao láº¡i lÃ  python ?
- CÃº phÃ¡p Ä‘Æ¡n giáº£n, dá»… Ä‘á»c, viáº¿t.
- ThÆ° viá»‡n AI cá»±c lá»›n giÃºp giáº£m thá»i gian code, cá»™ng Ä‘á»“ng lá»›n há»— trá»£ giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» gáº·p pháº£i.
- Táº­n dá»¥ng tá»‘t GPU báº±ng cÃ¡c thÆ° viá»‡n vÃ­ dá»¥ tensorflow, pytorch...
- VÃ  má»™t Ä‘iá»u quan trá»ng ná»¯a lÃ  code python ráº¥t ngáº¯n gá»n sáº½ hiá»‡u quáº£ khi generate code báº±ng cÃ¡c cÃ´ng cá»¥ hoáº·c model AI (copilot, gpt, gemini ...).

### Open AI(gpt4-o) + RAG (cáº§n cÃ³ open_ai_api_key)
#### open_ai_api_key cÃ³ thá»ƒ láº¥y báº±ng viá»‡c Ä‘Äƒng kÃ½ tÃ i khoáº£n vÃ  thanh toÃ¡n táº¡i https://platform.openai.com/

Clone project hoáº·c download trá»±c tiáº¿p file nÃ y vá» mÃ¡y:
https://github.com/phucly95/AI-Public/blob/main/openai-rag.py
Äá»ƒ cháº¡y Ä‘Æ°á»£c code nÃ y cáº§n cÃ i python 3.12 vÃ  sau Ä‘Ã³ cháº¡y thá»­ project, náº¿u bÃ¡o not found library nÃ o thÃ¬ cÃ i cÃ¡i Ä‘Ã³ lÃ  xong :D
```
python3 ./openai-rag.py # chá»‰nh láº¡i file path náº¿u cáº§n
```
### Deepseek R1 + RAG (Model opensource nhÆ°ng cáº§n mÃ¡y ram > 16gb Ä‘á»ƒ cháº¡y Ä‘Æ°á»£c).
- Äá»ƒ cháº¡y Ä‘Æ°á»£c code nÃ y cáº§n deeploy deepseek Ä‘á»ƒ lÃ m LLM. CÃ¡ch dá»… dÃ ng nháº¥t lÃ  cÃ i Ollama táº¡i https://ollama.com/download
- Sau khi cÃ i Ollama xong má»Ÿ terminal vÃ  cháº¡y lá»‡nh:
```
ollama run deepseek-r1:1.5b
```
Chá» cho model download vÃ  cháº¡y xong thÃ¬ model deepseek Ä‘Ã£ Ä‘Æ°á»£c deploy lÃªn http://localhost:11434
- Clone project hoáº·c download trá»±c tiáº¿p file nÃ y vá» mÃ¡y:
https://github.com/phucly95/AI-Public/blob/main/deepseek-rag.py
- Cháº¡y thá»­ vÃ  cÃ i cÃ¡c thÆ° viá»‡n cÃ²n thiáº¿u
```
python3 ./deepseek-rag.py
```

### Finetuning-LLM
...


Do bÃ i viáº¿t cÅ©ng Ä‘Ã£ dÃ i nÃªn em/mÃ¬nh xin phÃ©p Ä‘Æ°á»£c Ä‘á»ƒ láº¡i pháº§n nÃ y trong bÃ i viáº¿t riÃªng sau. Hy vá»ng sáº½ cÃ³ nhá»¯ng Ä‘Ã³ng gÃ³p cá»§a cÃ¡c anh chá»‹ em cÃ³ kinh nghiá»‡m vá» lÄ©nh vá»±c nÃ y Ä‘á»ƒ giÃºp em cáº£i thiá»‡n nhá»¯ng kiáº¿n thá»©c há»•ng, nhá»¯ng chá»— cÃ²n thiáº¿u xÃ³t hoáº·c hiá»ƒu sai. Em xin cáº£m Æ¡n !

# AI Agent & Langchain
Má»™t chá»§ Ä‘á» cÅ©ng ráº¥t hot gáº§n Ä‘Ã¢y mÃ  em muá»‘n chia sáº» cÃ¹ng má»i ngÆ°á»i, cháº¯c háº³n cÅ©ng nhiá»u anh chá»‹ em nghe Ä‘áº¿n AI Agent, Cursor, Manus ...

Váº­y AI Agent lÃ  gÃ¬ ? á»¨ng dá»¥ng nhÆ° tháº¿ nÃ o vÃ  lÃ m tháº¿ nÃ o Ä‘á»ƒ tá»± táº¡o cÃ¡c AI Agent cá»§a riÃªng mÃ¬nh?

CÃ¡c model opensource hiá»‡n nay cÃ³ sá»­ dá»¥ng lÃ m AI Agent Ä‘Æ°á»£c khÃ´ng ? VÃ  sá»­ dá»¥ng nhÆ° tháº¿ nÃ o ?

Anh chá»‹ em DE Ä‘Ã£ sá»­ dá»¥ng AI Agent nÃ o chÆ°a? cÃ¹ng tháº£o luáº­n á»Ÿ Ä‘Ã¢y nhÃ© !
